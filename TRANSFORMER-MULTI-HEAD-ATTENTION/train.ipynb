{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-12-17 10:41:03.459593: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow_datasets as tfds\n",
    "\n",
    "# Load the dataset\n",
    "examples, metadata = tfds.load('ted_hrlr_translate/pt_to_en', with_info=True, as_supervised=True)\n",
    "train_examples, val_examples = examples['train'], examples['validation']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-12-17 10:41:08.587866: W tensorflow/core/kernels/data/cache_dataset_ops.cc:858] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset  will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.\n",
      "2023-12-17 10:41:10.357340: W tensorflow/core/kernels/data/cache_dataset_ops.cc:858] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset  will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.\n"
     ]
    }
   ],
   "source": [
    "# Use a smaller subset for tokenizer training\n",
    "SUBSET_SIZE = 500  # for example\n",
    "\n",
    "# Create smaller datasets for building the tokenizers\n",
    "small_dataset_en = (en.numpy() for pt, en in train_examples.take(SUBSET_SIZE))\n",
    "small_dataset_pt = (pt.numpy() for pt, en in train_examples.take(SUBSET_SIZE))\n",
    "\n",
    "tokenizer_en = tfds.deprecated.text\n",
    "\n",
    "# Initialize a tokenizer with the desired vocabulary size and OOV token\n",
    "tokenizer_en = tokenizer_en.SubwordTextEncoder.build_from_corpus(\n",
    "    small_dataset_en, target_vocab_size=2**13\n",
    ")\n",
    "\n",
    "tokenizer_pt = tfds.deprecated.text\n",
    "\n",
    "# Initialize a tokenizer with the desired vocabulary size and OOV token\n",
    "tokenizer_pt = tokenizer_pt.SubwordTextEncoder.build_from_corpus(\n",
    "    small_dataset_pt, target_vocab_size=2**13\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "BUFFER_SIZE = 20000\n",
    "BATCH_SIZE = 64\n",
    "\n",
    "\n",
    "def encode(lang1, lang2):\n",
    "    lang1 = (\n",
    "        [tokenizer_pt.vocab_size]\n",
    "        + tokenizer_pt.encode(lang1.numpy())\n",
    "        + [tokenizer_pt.vocab_size + 1]\n",
    "    )\n",
    "\n",
    "    lang2 = (\n",
    "        [tokenizer_en.vocab_size]\n",
    "        + tokenizer_en.encode(lang2.numpy())\n",
    "        + [tokenizer_en.vocab_size + 1]\n",
    "    )\n",
    "\n",
    "    return lang1, lang2\n",
    "\n",
    "\n",
    "def tf_encode(pt, en):\n",
    "    result_pt, result_en = tf.py_function(encode, [pt, en], [tf.int64, tf.int64])\n",
    "    result_pt.set_shape([None])\n",
    "    result_en.set_shape([None])\n",
    "    return result_pt, result_en\n",
    "\n",
    "\n",
    "train_dataset = train_examples.map(tf_encode)\n",
    "train_dataset = train_dataset.cache()\n",
    "train_dataset = train_dataset.shuffle(BUFFER_SIZE).padded_batch(BATCH_SIZE)\n",
    "train_dataset = train_dataset.prefetch(tf.data.experimental.AUTOTUNE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from classes import Transformer\n",
    "\n",
    "num_layers = 4\n",
    "d_model = 128\n",
    "dff = 512\n",
    "num_heads = 8\n",
    "\n",
    "input_vocab_size = tokenizer_pt.vocab_size + 2\n",
    "target_vocab_size = tokenizer_en.vocab_size + 2\n",
    "dropout_rate = 0.1\n",
    "\n",
    "transformer = Transformer(\n",
    "    num_layers,\n",
    "    d_model,\n",
    "    num_heads,\n",
    "    dff,\n",
    "    input_vocab_size,\n",
    "    target_vocab_size,\n",
    "    pe_input=input_vocab_size,\n",
    "    pe_target=target_vocab_size,\n",
    "    rate=dropout_rate,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomSchedule(tf.keras.optimizers.schedules.LearningRateSchedule):\n",
    "    def __init__(self, d_model, warmup_steps=4000):\n",
    "        super(CustomSchedule, self).__init__()\n",
    "\n",
    "        self.d_model = d_model\n",
    "        self.d_model = tf.cast(self.d_model, tf.float32)\n",
    "        self.warmup_steps = warmup_steps\n",
    "\n",
    "    def __call__(self, step):\n",
    "        step = tf.cast(step, tf.float32)  # Cast step to float32\n",
    "        arg1 = tf.math.rsqrt(step)\n",
    "        arg2 = step * (self.warmup_steps**-1.5)\n",
    "\n",
    "        return tf.math.rsqrt(self.d_model) * tf.math.minimum(arg1, arg2)\n",
    "\n",
    "# Example usage\n",
    "learning_rate = CustomSchedule(d_model)\n",
    "\n",
    "optimizer = tf.keras.optimizers.Adam(\n",
    "    learning_rate, beta_1=0.9, beta_2=0.98, epsilon=1e-9\n",
    ")\n",
    "\n",
    "\n",
    "loss_object = tf.keras.losses.SparseCategoricalCrossentropy(\n",
    "    from_logits=True, reduction=\"none\"\n",
    ")\n",
    "\n",
    "\n",
    "def loss_function(real, pred):\n",
    "    mask = tf.math.logical_not(tf.math.equal(real, 0))\n",
    "    loss_ = loss_object(real, pred)\n",
    "\n",
    "    mask = tf.cast(mask, dtype=loss_.dtype)\n",
    "    loss_ *= mask\n",
    "\n",
    "    return tf.reduce_sum(loss_) / tf.reduce_sum(mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-12-17 10:41:22.847556: I tensorflow/core/kernels/data/shuffle_dataset_op.cc:422] ShuffleDatasetV3:38: Filling up shuffle buffer (this may take a while): 16587 of 20000\n",
      "2023-12-17 10:41:24.433952: I tensorflow/core/kernels/data/shuffle_dataset_op.cc:452] Shuffle buffer filled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of x after embedding: (64, 174, 128)\n",
      "Shape of x after scaling: (64, 174, 128)\n",
      "Shape of x after adding positional encoding: (64, 174, 128)\n",
      "After splitting heads: (64, 8, 174, 16) (64, 8, 174, 16) (64, 8, 174, 16)\n",
      "After scaled dot product attention: (64, 8, 174, 16)\n",
      "After concatenating heads: (64, 174, 128)\n",
      "EncoderLayer output: (64, 174, 128)\n",
      "After splitting heads: (64, 8, 174, 16) (64, 8, 174, 16) (64, 8, 174, 16)\n",
      "After scaled dot product attention: (64, 8, 174, 16)\n",
      "After concatenating heads: (64, 174, 128)\n",
      "EncoderLayer output: (64, 174, 128)\n",
      "After splitting heads: (64, 8, 174, 16) (64, 8, 174, 16) (64, 8, 174, 16)\n",
      "After scaled dot product attention: (64, 8, 174, 16)\n",
      "After concatenating heads: (64, 174, 128)\n",
      "EncoderLayer output: (64, 174, 128)\n",
      "After splitting heads: (64, 8, 174, 16) (64, 8, 174, 16) (64, 8, 174, 16)\n",
      "After scaled dot product attention: (64, 8, 174, 16)\n",
      "After concatenating heads: (64, 174, 128)\n",
      "EncoderLayer output: (64, 174, 128)\n",
      "Decoder After adding positional encoding: (64, 141, 128)\n",
      "After splitting heads: (64, 8, 141, 16) (64, 8, 141, 16) (64, 8, 141, 16)\n",
      "After scaled dot product attention: (64, 8, 141, 16)\n",
      "After concatenating heads: (64, 141, 128)\n",
      "After splitting heads: (64, 8, 141, 16) (64, 8, 174, 16) (64, 8, 174, 16)\n",
      "After scaled dot product attention: (64, 8, 141, 16)\n",
      "After concatenating heads: (64, 141, 128)\n",
      "DecoderLayer output: (64, 141, 128)\n",
      "After splitting heads: (64, 8, 141, 16) (64, 8, 141, 16) (64, 8, 141, 16)\n",
      "After scaled dot product attention: (64, 8, 141, 16)\n",
      "After concatenating heads: (64, 141, 128)\n",
      "After splitting heads: (64, 8, 141, 16) (64, 8, 174, 16) (64, 8, 174, 16)\n",
      "After scaled dot product attention: (64, 8, 141, 16)\n",
      "After concatenating heads: (64, 141, 128)\n",
      "DecoderLayer output: (64, 141, 128)\n",
      "After splitting heads: (64, 8, 141, 16) (64, 8, 141, 16) (64, 8, 141, 16)\n",
      "After scaled dot product attention: (64, 8, 141, 16)\n",
      "After concatenating heads: (64, 141, 128)\n",
      "After splitting heads: (64, 8, 141, 16) (64, 8, 174, 16) (64, 8, 174, 16)\n",
      "After scaled dot product attention: (64, 8, 141, 16)\n",
      "After concatenating heads: (64, 141, 128)\n",
      "DecoderLayer output: (64, 141, 128)\n",
      "After splitting heads: (64, 8, 141, 16) (64, 8, 141, 16) (64, 8, 141, 16)\n",
      "After scaled dot product attention: (64, 8, 141, 16)\n",
      "After concatenating heads: (64, 141, 128)\n",
      "After splitting heads: (64, 8, 141, 16) (64, 8, 174, 16) (64, 8, 174, 16)\n",
      "After scaled dot product attention: (64, 8, 141, 16)\n",
      "After concatenating heads: (64, 141, 128)\n",
      "DecoderLayer output: (64, 141, 128)\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['Variable:0', 'Variable:0', 'Variable:0', 'Variable:0', 'Variable:0', 'Variable:0', 'Variable:0', 'Variable:0', 'Variable:0', 'Variable:0', 'Variable:0', 'Variable:0', 'Variable:0', 'Variable:0', 'Variable:0', 'Variable:0', 'Variable:0', 'Variable:0', 'Variable:0', 'Variable:0', 'Variable:0', 'Variable:0', 'Variable:0', 'Variable:0', 'Variable:0', 'Variable:0', 'Variable:0', 'Variable:0', 'Variable:0', 'Variable:0', 'Variable:0', 'Variable:0', 'Variable:0', 'Variable:0', 'Variable:0', 'Variable:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss` argument?\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Gradients do not exist for variables ['Variable:0', 'Variable:0', 'Variable:0', 'Variable:0', 'Variable:0', 'Variable:0', 'Variable:0', 'Variable:0', 'Variable:0', 'Variable:0', 'Variable:0', 'Variable:0', 'Variable:0', 'Variable:0', 'Variable:0', 'Variable:0', 'Variable:0', 'Variable:0', 'Variable:0', 'Variable:0', 'Variable:0', 'Variable:0', 'Variable:0', 'Variable:0', 'Variable:0', 'Variable:0', 'Variable:0', 'Variable:0', 'Variable:0', 'Variable:0', 'Variable:0', 'Variable:0', 'Variable:0', 'Variable:0', 'Variable:0', 'Variable:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss` argument?\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Gradients do not exist for variables ['Variable:0', 'Variable:0', 'Variable:0', 'Variable:0', 'Variable:0', 'Variable:0', 'Variable:0', 'Variable:0', 'Variable:0', 'Variable:0', 'Variable:0', 'Variable:0', 'Variable:0', 'Variable:0', 'Variable:0', 'Variable:0', 'Variable:0', 'Variable:0', 'Variable:0', 'Variable:0', 'Variable:0', 'Variable:0', 'Variable:0', 'Variable:0', 'Variable:0', 'Variable:0', 'Variable:0', 'Variable:0', 'Variable:0', 'Variable:0', 'Variable:0', 'Variable:0', 'Variable:0', 'Variable:0', 'Variable:0', 'Variable:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss` argument?\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Gradients do not exist for variables ['Variable:0', 'Variable:0', 'Variable:0', 'Variable:0', 'Variable:0', 'Variable:0', 'Variable:0', 'Variable:0', 'Variable:0', 'Variable:0', 'Variable:0', 'Variable:0', 'Variable:0', 'Variable:0', 'Variable:0', 'Variable:0', 'Variable:0', 'Variable:0', 'Variable:0', 'Variable:0', 'Variable:0', 'Variable:0', 'Variable:0', 'Variable:0', 'Variable:0', 'Variable:0', 'Variable:0', 'Variable:0', 'Variable:0', 'Variable:0', 'Variable:0', 'Variable:0', 'Variable:0', 'Variable:0', 'Variable:0', 'Variable:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss` argument?\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 Batch 0 Loss 7.875532150268555\n",
      "Shape of x after embedding: (64, 427, 128)\n",
      "Shape of x after scaling: (64, 427, 128)\n",
      "Shape of x after adding positional encoding: (64, 427, 128)\n",
      "After splitting heads: (64, 8, 427, 16) (64, 8, 427, 16) (64, 8, 427, 16)\n",
      "After scaled dot product attention: (64, 8, 427, 16)\n",
      "After concatenating heads: (64, 427, 128)\n",
      "EncoderLayer output: (64, 427, 128)\n",
      "After splitting heads: (64, 8, 427, 16) (64, 8, 427, 16) (64, 8, 427, 16)\n",
      "After scaled dot product attention: (64, 8, 427, 16)\n",
      "After concatenating heads: (64, 427, 128)\n",
      "EncoderLayer output: (64, 427, 128)\n",
      "After splitting heads: (64, 8, 427, 16) (64, 8, 427, 16) (64, 8, 427, 16)\n",
      "After scaled dot product attention: (64, 8, 427, 16)\n",
      "After concatenating heads: (64, 427, 128)\n",
      "EncoderLayer output: (64, 427, 128)\n",
      "After splitting heads: (64, 8, 427, 16) (64, 8, 427, 16) (64, 8, 427, 16)\n",
      "After scaled dot product attention: (64, 8, 427, 16)\n",
      "After concatenating heads: (64, 427, 128)\n",
      "EncoderLayer output: (64, 427, 128)\n",
      "Decoder After adding positional encoding: (64, 408, 128)\n",
      "After splitting heads: (64, 8, 408, 16) (64, 8, 408, 16) (64, 8, 408, 16)\n",
      "After scaled dot product attention: (64, 8, 408, 16)\n",
      "After concatenating heads: (64, 408, 128)\n",
      "After splitting heads: (64, 8, 408, 16) (64, 8, 427, 16) (64, 8, 427, 16)\n",
      "After scaled dot product attention: (64, 8, 408, 16)\n",
      "After concatenating heads: (64, 408, 128)\n",
      "DecoderLayer output: (64, 408, 128)\n",
      "After splitting heads: (64, 8, 408, 16) (64, 8, 408, 16) (64, 8, 408, 16)\n",
      "After scaled dot product attention: (64, 8, 408, 16)\n",
      "After concatenating heads: (64, 408, 128)\n",
      "After splitting heads: (64, 8, 408, 16) (64, 8, 427, 16) (64, 8, 427, 16)\n",
      "After scaled dot product attention: (64, 8, 408, 16)\n",
      "After concatenating heads: (64, 408, 128)\n",
      "DecoderLayer output: (64, 408, 128)\n",
      "After splitting heads: (64, 8, 408, 16) (64, 8, 408, 16) (64, 8, 408, 16)\n",
      "After scaled dot product attention: (64, 8, 408, 16)\n",
      "After concatenating heads: (64, 408, 128)\n",
      "After splitting heads: (64, 8, 408, 16) (64, 8, 427, 16) (64, 8, 427, 16)\n",
      "After scaled dot product attention: (64, 8, 408, 16)\n",
      "After concatenating heads: (64, 408, 128)\n",
      "DecoderLayer output: (64, 408, 128)\n",
      "After splitting heads: (64, 8, 408, 16) (64, 8, 408, 16) (64, 8, 408, 16)\n",
      "After scaled dot product attention: (64, 8, 408, 16)\n",
      "After concatenating heads: (64, 408, 128)\n",
      "After splitting heads: (64, 8, 408, 16) (64, 8, 427, 16) (64, 8, 427, 16)\n",
      "After scaled dot product attention: (64, 8, 408, 16)\n",
      "After concatenating heads: (64, 408, 128)\n",
      "DecoderLayer output: (64, 408, 128)\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['Variable:0', 'Variable:0', 'Variable:0', 'Variable:0', 'Variable:0', 'Variable:0', 'Variable:0', 'Variable:0', 'Variable:0', 'Variable:0', 'Variable:0', 'Variable:0', 'Variable:0', 'Variable:0', 'Variable:0', 'Variable:0', 'Variable:0', 'Variable:0', 'Variable:0', 'Variable:0', 'Variable:0', 'Variable:0', 'Variable:0', 'Variable:0', 'Variable:0', 'Variable:0', 'Variable:0', 'Variable:0', 'Variable:0', 'Variable:0', 'Variable:0', 'Variable:0', 'Variable:0', 'Variable:0', 'Variable:0', 'Variable:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss` argument?\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Gradients do not exist for variables ['Variable:0', 'Variable:0', 'Variable:0', 'Variable:0', 'Variable:0', 'Variable:0', 'Variable:0', 'Variable:0', 'Variable:0', 'Variable:0', 'Variable:0', 'Variable:0', 'Variable:0', 'Variable:0', 'Variable:0', 'Variable:0', 'Variable:0', 'Variable:0', 'Variable:0', 'Variable:0', 'Variable:0', 'Variable:0', 'Variable:0', 'Variable:0', 'Variable:0', 'Variable:0', 'Variable:0', 'Variable:0', 'Variable:0', 'Variable:0', 'Variable:0', 'Variable:0', 'Variable:0', 'Variable:0', 'Variable:0', 'Variable:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss` argument?\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Gradients do not exist for variables ['Variable:0', 'Variable:0', 'Variable:0', 'Variable:0', 'Variable:0', 'Variable:0', 'Variable:0', 'Variable:0', 'Variable:0', 'Variable:0', 'Variable:0', 'Variable:0', 'Variable:0', 'Variable:0', 'Variable:0', 'Variable:0', 'Variable:0', 'Variable:0', 'Variable:0', 'Variable:0', 'Variable:0', 'Variable:0', 'Variable:0', 'Variable:0', 'Variable:0', 'Variable:0', 'Variable:0', 'Variable:0', 'Variable:0', 'Variable:0', 'Variable:0', 'Variable:0', 'Variable:0', 'Variable:0', 'Variable:0', 'Variable:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss` argument?\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Gradients do not exist for variables ['Variable:0', 'Variable:0', 'Variable:0', 'Variable:0', 'Variable:0', 'Variable:0', 'Variable:0', 'Variable:0', 'Variable:0', 'Variable:0', 'Variable:0', 'Variable:0', 'Variable:0', 'Variable:0', 'Variable:0', 'Variable:0', 'Variable:0', 'Variable:0', 'Variable:0', 'Variable:0', 'Variable:0', 'Variable:0', 'Variable:0', 'Variable:0', 'Variable:0', 'Variable:0', 'Variable:0', 'Variable:0', 'Variable:0', 'Variable:0', 'Variable:0', 'Variable:0', 'Variable:0', 'Variable:0', 'Variable:0', 'Variable:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss` argument?\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of x after embedding: (64, 152, 128)\n",
      "Shape of x after scaling: (64, 152, 128)\n",
      "Shape of x after adding positional encoding: (64, 152, 128)\n",
      "After splitting heads: (64, 8, 152, 16) (64, 8, 152, 16) (64, 8, 152, 16)\n",
      "After scaled dot product attention: (64, 8, 152, 16)\n",
      "After concatenating heads: (64, 152, 128)\n",
      "EncoderLayer output: (64, 152, 128)\n",
      "After splitting heads: (64, 8, 152, 16) (64, 8, 152, 16) (64, 8, 152, 16)\n",
      "After scaled dot product attention: (64, 8, 152, 16)\n",
      "After concatenating heads: (64, 152, 128)\n",
      "EncoderLayer output: (64, 152, 128)\n",
      "After splitting heads: (64, 8, 152, 16) (64, 8, 152, 16) (64, 8, 152, 16)\n",
      "After scaled dot product attention: (64, 8, 152, 16)\n",
      "After concatenating heads: (64, 152, 128)\n",
      "EncoderLayer output: (64, 152, 128)\n",
      "After splitting heads: (64, 8, 152, 16) (64, 8, 152, 16) (64, 8, 152, 16)\n",
      "After scaled dot product attention: (64, 8, 152, 16)\n",
      "After concatenating heads: (64, 152, 128)\n",
      "EncoderLayer output: (64, 152, 128)\n",
      "Decoder After adding positional encoding: (64, 138, 128)\n",
      "After splitting heads: (64, 8, 138, 16) (64, 8, 138, 16) (64, 8, 138, 16)\n",
      "After scaled dot product attention: (64, 8, 138, 16)\n",
      "After concatenating heads: (64, 138, 128)\n",
      "After splitting heads: (64, 8, 138, 16) (64, 8, 152, 16) (64, 8, 152, 16)\n",
      "After scaled dot product attention: (64, 8, 138, 16)\n",
      "After concatenating heads: (64, 138, 128)\n",
      "DecoderLayer output: (64, 138, 128)\n",
      "After splitting heads: (64, 8, 138, 16) (64, 8, 138, 16) (64, 8, 138, 16)\n",
      "After scaled dot product attention: (64, 8, 138, 16)\n",
      "After concatenating heads: (64, 138, 128)\n",
      "After splitting heads: (64, 8, 138, 16) (64, 8, 152, 16) (64, 8, 152, 16)\n",
      "After scaled dot product attention: (64, 8, 138, 16)\n",
      "After concatenating heads: (64, 138, 128)\n",
      "DecoderLayer output: (64, 138, 128)\n",
      "After splitting heads: (64, 8, 138, 16) (64, 8, 138, 16) (64, 8, 138, 16)\n",
      "After scaled dot product attention: (64, 8, 138, 16)\n",
      "After concatenating heads: (64, 138, 128)\n",
      "After splitting heads: (64, 8, 138, 16) (64, 8, 152, 16) (64, 8, 152, 16)\n",
      "After scaled dot product attention: (64, 8, 138, 16)\n",
      "After concatenating heads: (64, 138, 128)\n",
      "DecoderLayer output: (64, 138, 128)\n",
      "After splitting heads: (64, 8, 138, 16) (64, 8, 138, 16) (64, 8, 138, 16)\n",
      "After scaled dot product attention: (64, 8, 138, 16)\n",
      "After concatenating heads: (64, 138, 128)\n",
      "After splitting heads: (64, 8, 138, 16) (64, 8, 152, 16) (64, 8, 152, 16)\n",
      "After scaled dot product attention: (64, 8, 138, 16)\n",
      "After concatenating heads: (64, 138, 128)\n",
      "DecoderLayer output: (64, 138, 128)\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['Variable:0', 'Variable:0', 'Variable:0', 'Variable:0', 'Variable:0', 'Variable:0', 'Variable:0', 'Variable:0', 'Variable:0', 'Variable:0', 'Variable:0', 'Variable:0', 'Variable:0', 'Variable:0', 'Variable:0', 'Variable:0', 'Variable:0', 'Variable:0', 'Variable:0', 'Variable:0', 'Variable:0', 'Variable:0', 'Variable:0', 'Variable:0', 'Variable:0', 'Variable:0', 'Variable:0', 'Variable:0', 'Variable:0', 'Variable:0', 'Variable:0', 'Variable:0', 'Variable:0', 'Variable:0', 'Variable:0', 'Variable:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss` argument?\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Gradients do not exist for variables ['Variable:0', 'Variable:0', 'Variable:0', 'Variable:0', 'Variable:0', 'Variable:0', 'Variable:0', 'Variable:0', 'Variable:0', 'Variable:0', 'Variable:0', 'Variable:0', 'Variable:0', 'Variable:0', 'Variable:0', 'Variable:0', 'Variable:0', 'Variable:0', 'Variable:0', 'Variable:0', 'Variable:0', 'Variable:0', 'Variable:0', 'Variable:0', 'Variable:0', 'Variable:0', 'Variable:0', 'Variable:0', 'Variable:0', 'Variable:0', 'Variable:0', 'Variable:0', 'Variable:0', 'Variable:0', 'Variable:0', 'Variable:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss` argument?\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Gradients do not exist for variables ['Variable:0', 'Variable:0', 'Variable:0', 'Variable:0', 'Variable:0', 'Variable:0', 'Variable:0', 'Variable:0', 'Variable:0', 'Variable:0', 'Variable:0', 'Variable:0', 'Variable:0', 'Variable:0', 'Variable:0', 'Variable:0', 'Variable:0', 'Variable:0', 'Variable:0', 'Variable:0', 'Variable:0', 'Variable:0', 'Variable:0', 'Variable:0', 'Variable:0', 'Variable:0', 'Variable:0', 'Variable:0', 'Variable:0', 'Variable:0', 'Variable:0', 'Variable:0', 'Variable:0', 'Variable:0', 'Variable:0', 'Variable:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss` argument?\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Gradients do not exist for variables ['Variable:0', 'Variable:0', 'Variable:0', 'Variable:0', 'Variable:0', 'Variable:0', 'Variable:0', 'Variable:0', 'Variable:0', 'Variable:0', 'Variable:0', 'Variable:0', 'Variable:0', 'Variable:0', 'Variable:0', 'Variable:0', 'Variable:0', 'Variable:0', 'Variable:0', 'Variable:0', 'Variable:0', 'Variable:0', 'Variable:0', 'Variable:0', 'Variable:0', 'Variable:0', 'Variable:0', 'Variable:0', 'Variable:0', 'Variable:0', 'Variable:0', 'Variable:0', 'Variable:0', 'Variable:0', 'Variable:0', 'Variable:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss` argument?\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of x after embedding: (64, 110, 128)\n",
      "Shape of x after scaling: (64, 110, 128)\n",
      "Shape of x after adding positional encoding: (64, 110, 128)\n",
      "After splitting heads: (64, 8, 110, 16) (64, 8, 110, 16) (64, 8, 110, 16)\n",
      "After scaled dot product attention: (64, 8, 110, 16)\n",
      "After concatenating heads: (64, 110, 128)\n",
      "EncoderLayer output: (64, 110, 128)\n",
      "After splitting heads: (64, 8, 110, 16) (64, 8, 110, 16) (64, 8, 110, 16)\n",
      "After scaled dot product attention: (64, 8, 110, 16)\n",
      "After concatenating heads: (64, 110, 128)\n",
      "EncoderLayer output: (64, 110, 128)\n",
      "After splitting heads: (64, 8, 110, 16) (64, 8, 110, 16) (64, 8, 110, 16)\n",
      "After scaled dot product attention: (64, 8, 110, 16)\n",
      "After concatenating heads: (64, 110, 128)\n",
      "EncoderLayer output: (64, 110, 128)\n",
      "After splitting heads: (64, 8, 110, 16) (64, 8, 110, 16) (64, 8, 110, 16)\n",
      "After scaled dot product attention: (64, 8, 110, 16)\n",
      "After concatenating heads: (64, 110, 128)\n",
      "EncoderLayer output: (64, 110, 128)\n",
      "Decoder After adding positional encoding: (64, 85, 128)\n",
      "After splitting heads: (64, 8, 85, 16) (64, 8, 85, 16) (64, 8, 85, 16)\n",
      "After scaled dot product attention: (64, 8, 85, 16)\n",
      "After concatenating heads: (64, 85, 128)\n",
      "After splitting heads: (64, 8, 85, 16) (64, 8, 110, 16) (64, 8, 110, 16)\n",
      "After scaled dot product attention: (64, 8, 85, 16)\n",
      "After concatenating heads: (64, 85, 128)\n",
      "DecoderLayer output: (64, 85, 128)\n",
      "After splitting heads: (64, 8, 85, 16) (64, 8, 85, 16) (64, 8, 85, 16)\n",
      "After scaled dot product attention: (64, 8, 85, 16)\n",
      "After concatenating heads: (64, 85, 128)\n",
      "After splitting heads: (64, 8, 85, 16) (64, 8, 110, 16) (64, 8, 110, 16)\n",
      "After scaled dot product attention: (64, 8, 85, 16)\n",
      "After concatenating heads: (64, 85, 128)\n",
      "DecoderLayer output: (64, 85, 128)\n",
      "After splitting heads: (64, 8, 85, 16) (64, 8, 85, 16) (64, 8, 85, 16)\n",
      "After scaled dot product attention: (64, 8, 85, 16)\n",
      "After concatenating heads: (64, 85, 128)\n",
      "After splitting heads: (64, 8, 85, 16) (64, 8, 110, 16) (64, 8, 110, 16)\n",
      "After scaled dot product attention: (64, 8, 85, 16)\n",
      "After concatenating heads: (64, 85, 128)\n",
      "DecoderLayer output: (64, 85, 128)\n",
      "After splitting heads: (64, 8, 85, 16) (64, 8, 85, 16) (64, 8, 85, 16)\n",
      "After scaled dot product attention: (64, 8, 85, 16)\n",
      "After concatenating heads: (64, 85, 128)\n",
      "After splitting heads: (64, 8, 85, 16) (64, 8, 110, 16) (64, 8, 110, 16)\n",
      "After scaled dot product attention: (64, 8, 85, 16)\n",
      "After concatenating heads: (64, 85, 128)\n",
      "DecoderLayer output: (64, 85, 128)\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['Variable:0', 'Variable:0', 'Variable:0', 'Variable:0', 'Variable:0', 'Variable:0', 'Variable:0', 'Variable:0', 'Variable:0', 'Variable:0', 'Variable:0', 'Variable:0', 'Variable:0', 'Variable:0', 'Variable:0', 'Variable:0', 'Variable:0', 'Variable:0', 'Variable:0', 'Variable:0', 'Variable:0', 'Variable:0', 'Variable:0', 'Variable:0', 'Variable:0', 'Variable:0', 'Variable:0', 'Variable:0', 'Variable:0', 'Variable:0', 'Variable:0', 'Variable:0', 'Variable:0', 'Variable:0', 'Variable:0', 'Variable:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss` argument?\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Gradients do not exist for variables ['Variable:0', 'Variable:0', 'Variable:0', 'Variable:0', 'Variable:0', 'Variable:0', 'Variable:0', 'Variable:0', 'Variable:0', 'Variable:0', 'Variable:0', 'Variable:0', 'Variable:0', 'Variable:0', 'Variable:0', 'Variable:0', 'Variable:0', 'Variable:0', 'Variable:0', 'Variable:0', 'Variable:0', 'Variable:0', 'Variable:0', 'Variable:0', 'Variable:0', 'Variable:0', 'Variable:0', 'Variable:0', 'Variable:0', 'Variable:0', 'Variable:0', 'Variable:0', 'Variable:0', 'Variable:0', 'Variable:0', 'Variable:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss` argument?\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Gradients do not exist for variables ['Variable:0', 'Variable:0', 'Variable:0', 'Variable:0', 'Variable:0', 'Variable:0', 'Variable:0', 'Variable:0', 'Variable:0', 'Variable:0', 'Variable:0', 'Variable:0', 'Variable:0', 'Variable:0', 'Variable:0', 'Variable:0', 'Variable:0', 'Variable:0', 'Variable:0', 'Variable:0', 'Variable:0', 'Variable:0', 'Variable:0', 'Variable:0', 'Variable:0', 'Variable:0', 'Variable:0', 'Variable:0', 'Variable:0', 'Variable:0', 'Variable:0', 'Variable:0', 'Variable:0', 'Variable:0', 'Variable:0', 'Variable:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss` argument?\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Gradients do not exist for variables ['Variable:0', 'Variable:0', 'Variable:0', 'Variable:0', 'Variable:0', 'Variable:0', 'Variable:0', 'Variable:0', 'Variable:0', 'Variable:0', 'Variable:0', 'Variable:0', 'Variable:0', 'Variable:0', 'Variable:0', 'Variable:0', 'Variable:0', 'Variable:0', 'Variable:0', 'Variable:0', 'Variable:0', 'Variable:0', 'Variable:0', 'Variable:0', 'Variable:0', 'Variable:0', 'Variable:0', 'Variable:0', 'Variable:0', 'Variable:0', 'Variable:0', 'Variable:0', 'Variable:0', 'Variable:0', 'Variable:0', 'Variable:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss` argument?\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of x after embedding: (64, 106, 128)\n",
      "Shape of x after scaling: (64, 106, 128)\n",
      "Shape of x after adding positional encoding: (64, 106, 128)\n",
      "After splitting heads: (64, 8, 106, 16) (64, 8, 106, 16) (64, 8, 106, 16)\n",
      "After scaled dot product attention: (64, 8, 106, 16)\n",
      "After concatenating heads: (64, 106, 128)\n",
      "EncoderLayer output: (64, 106, 128)\n",
      "After splitting heads: (64, 8, 106, 16) (64, 8, 106, 16) (64, 8, 106, 16)\n",
      "After scaled dot product attention: (64, 8, 106, 16)\n",
      "After concatenating heads: (64, 106, 128)\n",
      "EncoderLayer output: (64, 106, 128)\n",
      "After splitting heads: (64, 8, 106, 16) (64, 8, 106, 16) (64, 8, 106, 16)\n",
      "After scaled dot product attention: (64, 8, 106, 16)\n",
      "After concatenating heads: (64, 106, 128)\n",
      "EncoderLayer output: (64, 106, 128)\n",
      "After splitting heads: (64, 8, 106, 16) (64, 8, 106, 16) (64, 8, 106, 16)\n",
      "After scaled dot product attention: (64, 8, 106, 16)\n",
      "After concatenating heads: (64, 106, 128)\n",
      "EncoderLayer output: (64, 106, 128)\n",
      "Decoder After adding positional encoding: (64, 106, 128)\n",
      "After splitting heads: (64, 8, 106, 16) (64, 8, 106, 16) (64, 8, 106, 16)\n",
      "After scaled dot product attention: (64, 8, 106, 16)\n",
      "After concatenating heads: (64, 106, 128)\n",
      "After splitting heads: (64, 8, 106, 16) (64, 8, 106, 16) (64, 8, 106, 16)\n",
      "After scaled dot product attention: (64, 8, 106, 16)\n",
      "After concatenating heads: (64, 106, 128)\n",
      "DecoderLayer output: (64, 106, 128)\n",
      "After splitting heads: (64, 8, 106, 16) (64, 8, 106, 16) (64, 8, 106, 16)\n",
      "After scaled dot product attention: (64, 8, 106, 16)\n",
      "After concatenating heads: (64, 106, 128)\n",
      "After splitting heads: (64, 8, 106, 16) (64, 8, 106, 16) (64, 8, 106, 16)\n",
      "After scaled dot product attention: (64, 8, 106, 16)\n",
      "After concatenating heads: (64, 106, 128)\n",
      "DecoderLayer output: (64, 106, 128)\n",
      "After splitting heads: (64, 8, 106, 16) (64, 8, 106, 16) (64, 8, 106, 16)\n",
      "After scaled dot product attention: (64, 8, 106, 16)\n",
      "After concatenating heads: (64, 106, 128)\n",
      "After splitting heads: (64, 8, 106, 16) (64, 8, 106, 16) (64, 8, 106, 16)\n",
      "After scaled dot product attention: (64, 8, 106, 16)\n",
      "After concatenating heads: (64, 106, 128)\n",
      "DecoderLayer output: (64, 106, 128)\n",
      "After splitting heads: (64, 8, 106, 16) (64, 8, 106, 16) (64, 8, 106, 16)\n",
      "After scaled dot product attention: (64, 8, 106, 16)\n",
      "After concatenating heads: (64, 106, 128)\n",
      "After splitting heads: (64, 8, 106, 16) (64, 8, 106, 16) (64, 8, 106, 16)\n",
      "After scaled dot product attention: (64, 8, 106, 16)\n",
      "After concatenating heads: (64, 106, 128)\n",
      "DecoderLayer output: (64, 106, 128)\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['Variable:0', 'Variable:0', 'Variable:0', 'Variable:0', 'Variable:0', 'Variable:0', 'Variable:0', 'Variable:0', 'Variable:0', 'Variable:0', 'Variable:0', 'Variable:0', 'Variable:0', 'Variable:0', 'Variable:0', 'Variable:0', 'Variable:0', 'Variable:0', 'Variable:0', 'Variable:0', 'Variable:0', 'Variable:0', 'Variable:0', 'Variable:0', 'Variable:0', 'Variable:0', 'Variable:0', 'Variable:0', 'Variable:0', 'Variable:0', 'Variable:0', 'Variable:0', 'Variable:0', 'Variable:0', 'Variable:0', 'Variable:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss` argument?\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Gradients do not exist for variables ['Variable:0', 'Variable:0', 'Variable:0', 'Variable:0', 'Variable:0', 'Variable:0', 'Variable:0', 'Variable:0', 'Variable:0', 'Variable:0', 'Variable:0', 'Variable:0', 'Variable:0', 'Variable:0', 'Variable:0', 'Variable:0', 'Variable:0', 'Variable:0', 'Variable:0', 'Variable:0', 'Variable:0', 'Variable:0', 'Variable:0', 'Variable:0', 'Variable:0', 'Variable:0', 'Variable:0', 'Variable:0', 'Variable:0', 'Variable:0', 'Variable:0', 'Variable:0', 'Variable:0', 'Variable:0', 'Variable:0', 'Variable:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss` argument?\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Gradients do not exist for variables ['Variable:0', 'Variable:0', 'Variable:0', 'Variable:0', 'Variable:0', 'Variable:0', 'Variable:0', 'Variable:0', 'Variable:0', 'Variable:0', 'Variable:0', 'Variable:0', 'Variable:0', 'Variable:0', 'Variable:0', 'Variable:0', 'Variable:0', 'Variable:0', 'Variable:0', 'Variable:0', 'Variable:0', 'Variable:0', 'Variable:0', 'Variable:0', 'Variable:0', 'Variable:0', 'Variable:0', 'Variable:0', 'Variable:0', 'Variable:0', 'Variable:0', 'Variable:0', 'Variable:0', 'Variable:0', 'Variable:0', 'Variable:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss` argument?\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Gradients do not exist for variables ['Variable:0', 'Variable:0', 'Variable:0', 'Variable:0', 'Variable:0', 'Variable:0', 'Variable:0', 'Variable:0', 'Variable:0', 'Variable:0', 'Variable:0', 'Variable:0', 'Variable:0', 'Variable:0', 'Variable:0', 'Variable:0', 'Variable:0', 'Variable:0', 'Variable:0', 'Variable:0', 'Variable:0', 'Variable:0', 'Variable:0', 'Variable:0', 'Variable:0', 'Variable:0', 'Variable:0', 'Variable:0', 'Variable:0', 'Variable:0', 'Variable:0', 'Variable:0', 'Variable:0', 'Variable:0', 'Variable:0', 'Variable:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss` argument?\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of x after embedding: (64, 129, 128)\n",
      "Shape of x after scaling: (64, 129, 128)\n",
      "Shape of x after adding positional encoding: (64, 129, 128)\n",
      "After splitting heads: (64, 8, 129, 16) (64, 8, 129, 16) (64, 8, 129, 16)\n",
      "After scaled dot product attention: (64, 8, 129, 16)\n",
      "After concatenating heads: (64, 129, 128)\n",
      "EncoderLayer output: (64, 129, 128)\n",
      "After splitting heads: (64, 8, 129, 16) (64, 8, 129, 16) (64, 8, 129, 16)\n",
      "After scaled dot product attention: (64, 8, 129, 16)\n",
      "After concatenating heads: (64, 129, 128)\n",
      "EncoderLayer output: (64, 129, 128)\n",
      "After splitting heads: (64, 8, 129, 16) (64, 8, 129, 16) (64, 8, 129, 16)\n",
      "After scaled dot product attention: (64, 8, 129, 16)\n",
      "After concatenating heads: (64, 129, 128)\n",
      "EncoderLayer output: (64, 129, 128)\n",
      "After splitting heads: (64, 8, 129, 16) (64, 8, 129, 16) (64, 8, 129, 16)\n",
      "After scaled dot product attention: (64, 8, 129, 16)\n",
      "After concatenating heads: (64, 129, 128)\n",
      "EncoderLayer output: (64, 129, 128)\n",
      "Decoder After adding positional encoding: (64, 124, 128)\n",
      "After splitting heads: (64, 8, 124, 16) (64, 8, 124, 16) (64, 8, 124, 16)\n",
      "After scaled dot product attention: (64, 8, 124, 16)\n",
      "After concatenating heads: (64, 124, 128)\n",
      "After splitting heads: (64, 8, 124, 16) (64, 8, 129, 16) (64, 8, 129, 16)\n",
      "After scaled dot product attention: (64, 8, 124, 16)\n",
      "After concatenating heads: (64, 124, 128)\n",
      "DecoderLayer output: (64, 124, 128)\n",
      "After splitting heads: (64, 8, 124, 16) (64, 8, 124, 16) (64, 8, 124, 16)\n",
      "After scaled dot product attention: (64, 8, 124, 16)\n",
      "After concatenating heads: (64, 124, 128)\n",
      "After splitting heads: (64, 8, 124, 16) (64, 8, 129, 16) (64, 8, 129, 16)\n",
      "After scaled dot product attention: (64, 8, 124, 16)\n",
      "After concatenating heads: (64, 124, 128)\n",
      "DecoderLayer output: (64, 124, 128)\n",
      "After splitting heads: (64, 8, 124, 16) (64, 8, 124, 16) (64, 8, 124, 16)\n",
      "After scaled dot product attention: (64, 8, 124, 16)\n",
      "After concatenating heads: (64, 124, 128)\n",
      "After splitting heads: (64, 8, 124, 16) (64, 8, 129, 16) (64, 8, 129, 16)\n",
      "After scaled dot product attention: (64, 8, 124, 16)\n",
      "After concatenating heads: (64, 124, 128)\n",
      "DecoderLayer output: (64, 124, 128)\n",
      "After splitting heads: (64, 8, 124, 16) (64, 8, 124, 16) (64, 8, 124, 16)\n",
      "After scaled dot product attention: (64, 8, 124, 16)\n",
      "After concatenating heads: (64, 124, 128)\n",
      "After splitting heads: (64, 8, 124, 16) (64, 8, 129, 16) (64, 8, 129, 16)\n",
      "After scaled dot product attention: (64, 8, 124, 16)\n",
      "After concatenating heads: (64, 124, 128)\n",
      "DecoderLayer output: (64, 124, 128)\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['Variable:0', 'Variable:0', 'Variable:0', 'Variable:0', 'Variable:0', 'Variable:0', 'Variable:0', 'Variable:0', 'Variable:0', 'Variable:0', 'Variable:0', 'Variable:0', 'Variable:0', 'Variable:0', 'Variable:0', 'Variable:0', 'Variable:0', 'Variable:0', 'Variable:0', 'Variable:0', 'Variable:0', 'Variable:0', 'Variable:0', 'Variable:0', 'Variable:0', 'Variable:0', 'Variable:0', 'Variable:0', 'Variable:0', 'Variable:0', 'Variable:0', 'Variable:0', 'Variable:0', 'Variable:0', 'Variable:0', 'Variable:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss` argument?\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Gradients do not exist for variables ['Variable:0', 'Variable:0', 'Variable:0', 'Variable:0', 'Variable:0', 'Variable:0', 'Variable:0', 'Variable:0', 'Variable:0', 'Variable:0', 'Variable:0', 'Variable:0', 'Variable:0', 'Variable:0', 'Variable:0', 'Variable:0', 'Variable:0', 'Variable:0', 'Variable:0', 'Variable:0', 'Variable:0', 'Variable:0', 'Variable:0', 'Variable:0', 'Variable:0', 'Variable:0', 'Variable:0', 'Variable:0', 'Variable:0', 'Variable:0', 'Variable:0', 'Variable:0', 'Variable:0', 'Variable:0', 'Variable:0', 'Variable:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss` argument?\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Gradients do not exist for variables ['Variable:0', 'Variable:0', 'Variable:0', 'Variable:0', 'Variable:0', 'Variable:0', 'Variable:0', 'Variable:0', 'Variable:0', 'Variable:0', 'Variable:0', 'Variable:0', 'Variable:0', 'Variable:0', 'Variable:0', 'Variable:0', 'Variable:0', 'Variable:0', 'Variable:0', 'Variable:0', 'Variable:0', 'Variable:0', 'Variable:0', 'Variable:0', 'Variable:0', 'Variable:0', 'Variable:0', 'Variable:0', 'Variable:0', 'Variable:0', 'Variable:0', 'Variable:0', 'Variable:0', 'Variable:0', 'Variable:0', 'Variable:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss` argument?\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Gradients do not exist for variables ['Variable:0', 'Variable:0', 'Variable:0', 'Variable:0', 'Variable:0', 'Variable:0', 'Variable:0', 'Variable:0', 'Variable:0', 'Variable:0', 'Variable:0', 'Variable:0', 'Variable:0', 'Variable:0', 'Variable:0', 'Variable:0', 'Variable:0', 'Variable:0', 'Variable:0', 'Variable:0', 'Variable:0', 'Variable:0', 'Variable:0', 'Variable:0', 'Variable:0', 'Variable:0', 'Variable:0', 'Variable:0', 'Variable:0', 'Variable:0', 'Variable:0', 'Variable:0', 'Variable:0', 'Variable:0', 'Variable:0', 'Variable:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss` argument?\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of x after embedding: (64, 159, 128)\n",
      "Shape of x after scaling: (64, 159, 128)\n",
      "Shape of x after adding positional encoding: (64, 159, 128)\n",
      "After splitting heads: (64, 8, 159, 16) (64, 8, 159, 16) (64, 8, 159, 16)\n",
      "After scaled dot product attention: (64, 8, 159, 16)\n",
      "After concatenating heads: (64, 159, 128)\n",
      "EncoderLayer output: (64, 159, 128)\n",
      "After splitting heads: (64, 8, 159, 16) (64, 8, 159, 16) (64, 8, 159, 16)\n",
      "After scaled dot product attention: (64, 8, 159, 16)\n",
      "After concatenating heads: (64, 159, 128)\n",
      "EncoderLayer output: (64, 159, 128)\n",
      "After splitting heads: (64, 8, 159, 16) (64, 8, 159, 16) (64, 8, 159, 16)\n",
      "After scaled dot product attention: (64, 8, 159, 16)\n",
      "After concatenating heads: (64, 159, 128)\n",
      "EncoderLayer output: (64, 159, 128)\n",
      "After splitting heads: (64, 8, 159, 16) (64, 8, 159, 16) (64, 8, 159, 16)\n",
      "After scaled dot product attention: (64, 8, 159, 16)\n",
      "After concatenating heads: (64, 159, 128)\n",
      "EncoderLayer output: (64, 159, 128)\n",
      "Decoder After adding positional encoding: (64, 132, 128)\n",
      "After splitting heads: (64, 8, 132, 16) (64, 8, 132, 16) (64, 8, 132, 16)\n",
      "After scaled dot product attention: (64, 8, 132, 16)\n",
      "After concatenating heads: (64, 132, 128)\n",
      "After splitting heads: (64, 8, 132, 16) (64, 8, 159, 16) (64, 8, 159, 16)\n",
      "After scaled dot product attention: (64, 8, 132, 16)\n",
      "After concatenating heads: (64, 132, 128)\n",
      "DecoderLayer output: (64, 132, 128)\n",
      "After splitting heads: (64, 8, 132, 16) (64, 8, 132, 16) (64, 8, 132, 16)\n",
      "After scaled dot product attention: (64, 8, 132, 16)\n",
      "After concatenating heads: (64, 132, 128)\n",
      "After splitting heads: (64, 8, 132, 16) (64, 8, 159, 16) (64, 8, 159, 16)\n",
      "After scaled dot product attention: (64, 8, 132, 16)\n",
      "After concatenating heads: (64, 132, 128)\n",
      "DecoderLayer output: (64, 132, 128)\n",
      "After splitting heads: (64, 8, 132, 16) (64, 8, 132, 16) (64, 8, 132, 16)\n",
      "After scaled dot product attention: (64, 8, 132, 16)\n",
      "After concatenating heads: (64, 132, 128)\n",
      "After splitting heads: (64, 8, 132, 16) (64, 8, 159, 16) (64, 8, 159, 16)\n",
      "After scaled dot product attention: (64, 8, 132, 16)\n",
      "After concatenating heads: (64, 132, 128)\n",
      "DecoderLayer output: (64, 132, 128)\n",
      "After splitting heads: (64, 8, 132, 16) (64, 8, 132, 16) (64, 8, 132, 16)\n",
      "After scaled dot product attention: (64, 8, 132, 16)\n",
      "After concatenating heads: (64, 132, 128)\n",
      "After splitting heads: (64, 8, 132, 16) (64, 8, 159, 16) (64, 8, 159, 16)\n",
      "After scaled dot product attention: (64, 8, 132, 16)\n",
      "After concatenating heads: (64, 132, 128)\n",
      "DecoderLayer output: (64, 132, 128)\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['Variable:0', 'Variable:0', 'Variable:0', 'Variable:0', 'Variable:0', 'Variable:0', 'Variable:0', 'Variable:0', 'Variable:0', 'Variable:0', 'Variable:0', 'Variable:0', 'Variable:0', 'Variable:0', 'Variable:0', 'Variable:0', 'Variable:0', 'Variable:0', 'Variable:0', 'Variable:0', 'Variable:0', 'Variable:0', 'Variable:0', 'Variable:0', 'Variable:0', 'Variable:0', 'Variable:0', 'Variable:0', 'Variable:0', 'Variable:0', 'Variable:0', 'Variable:0', 'Variable:0', 'Variable:0', 'Variable:0', 'Variable:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss` argument?\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Gradients do not exist for variables ['Variable:0', 'Variable:0', 'Variable:0', 'Variable:0', 'Variable:0', 'Variable:0', 'Variable:0', 'Variable:0', 'Variable:0', 'Variable:0', 'Variable:0', 'Variable:0', 'Variable:0', 'Variable:0', 'Variable:0', 'Variable:0', 'Variable:0', 'Variable:0', 'Variable:0', 'Variable:0', 'Variable:0', 'Variable:0', 'Variable:0', 'Variable:0', 'Variable:0', 'Variable:0', 'Variable:0', 'Variable:0', 'Variable:0', 'Variable:0', 'Variable:0', 'Variable:0', 'Variable:0', 'Variable:0', 'Variable:0', 'Variable:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss` argument?\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Gradients do not exist for variables ['Variable:0', 'Variable:0', 'Variable:0', 'Variable:0', 'Variable:0', 'Variable:0', 'Variable:0', 'Variable:0', 'Variable:0', 'Variable:0', 'Variable:0', 'Variable:0', 'Variable:0', 'Variable:0', 'Variable:0', 'Variable:0', 'Variable:0', 'Variable:0', 'Variable:0', 'Variable:0', 'Variable:0', 'Variable:0', 'Variable:0', 'Variable:0', 'Variable:0', 'Variable:0', 'Variable:0', 'Variable:0', 'Variable:0', 'Variable:0', 'Variable:0', 'Variable:0', 'Variable:0', 'Variable:0', 'Variable:0', 'Variable:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss` argument?\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Gradients do not exist for variables ['Variable:0', 'Variable:0', 'Variable:0', 'Variable:0', 'Variable:0', 'Variable:0', 'Variable:0', 'Variable:0', 'Variable:0', 'Variable:0', 'Variable:0', 'Variable:0', 'Variable:0', 'Variable:0', 'Variable:0', 'Variable:0', 'Variable:0', 'Variable:0', 'Variable:0', 'Variable:0', 'Variable:0', 'Variable:0', 'Variable:0', 'Variable:0', 'Variable:0', 'Variable:0', 'Variable:0', 'Variable:0', 'Variable:0', 'Variable:0', 'Variable:0', 'Variable:0', 'Variable:0', 'Variable:0', 'Variable:0', 'Variable:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss` argument?\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of x after embedding: (64, 225, 128)\n",
      "Shape of x after scaling: (64, 225, 128)\n",
      "Shape of x after adding positional encoding: (64, 225, 128)\n",
      "After splitting heads: (64, 8, 225, 16) (64, 8, 225, 16) (64, 8, 225, 16)\n",
      "After scaled dot product attention: (64, 8, 225, 16)\n",
      "After concatenating heads: (64, 225, 128)\n",
      "EncoderLayer output: (64, 225, 128)\n",
      "After splitting heads: (64, 8, 225, 16) (64, 8, 225, 16) (64, 8, 225, 16)\n",
      "After scaled dot product attention: (64, 8, 225, 16)\n",
      "After concatenating heads: (64, 225, 128)\n",
      "EncoderLayer output: (64, 225, 128)\n",
      "After splitting heads: (64, 8, 225, 16) (64, 8, 225, 16) (64, 8, 225, 16)\n",
      "After scaled dot product attention: (64, 8, 225, 16)\n",
      "After concatenating heads: (64, 225, 128)\n",
      "EncoderLayer output: (64, 225, 128)\n",
      "After splitting heads: (64, 8, 225, 16) (64, 8, 225, 16) (64, 8, 225, 16)\n",
      "After scaled dot product attention: (64, 8, 225, 16)\n",
      "After concatenating heads: (64, 225, 128)\n",
      "EncoderLayer output: (64, 225, 128)\n",
      "Decoder After adding positional encoding: (64, 210, 128)\n",
      "After splitting heads: (64, 8, 210, 16) (64, 8, 210, 16) (64, 8, 210, 16)\n",
      "After scaled dot product attention: (64, 8, 210, 16)\n",
      "After concatenating heads: (64, 210, 128)\n",
      "After splitting heads: (64, 8, 210, 16) (64, 8, 225, 16) (64, 8, 225, 16)\n",
      "After scaled dot product attention: (64, 8, 210, 16)\n",
      "After concatenating heads: (64, 210, 128)\n",
      "DecoderLayer output: (64, 210, 128)\n",
      "After splitting heads: (64, 8, 210, 16) (64, 8, 210, 16) (64, 8, 210, 16)\n",
      "After scaled dot product attention: (64, 8, 210, 16)\n",
      "After concatenating heads: (64, 210, 128)\n",
      "After splitting heads: (64, 8, 210, 16) (64, 8, 225, 16) (64, 8, 225, 16)\n",
      "After scaled dot product attention: (64, 8, 210, 16)\n",
      "After concatenating heads: (64, 210, 128)\n",
      "DecoderLayer output: (64, 210, 128)\n",
      "After splitting heads: (64, 8, 210, 16) (64, 8, 210, 16) (64, 8, 210, 16)\n",
      "After scaled dot product attention: (64, 8, 210, 16)\n",
      "After concatenating heads: (64, 210, 128)\n",
      "After splitting heads: (64, 8, 210, 16) (64, 8, 225, 16) (64, 8, 225, 16)\n",
      "After scaled dot product attention: (64, 8, 210, 16)\n",
      "After concatenating heads: (64, 210, 128)\n",
      "DecoderLayer output: (64, 210, 128)\n",
      "After splitting heads: (64, 8, 210, 16) (64, 8, 210, 16) (64, 8, 210, 16)\n",
      "After scaled dot product attention: (64, 8, 210, 16)\n",
      "After concatenating heads: (64, 210, 128)\n",
      "After splitting heads: (64, 8, 210, 16) (64, 8, 225, 16) (64, 8, 225, 16)\n",
      "After scaled dot product attention: (64, 8, 210, 16)\n",
      "After concatenating heads: (64, 210, 128)\n",
      "DecoderLayer output: (64, 210, 128)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-12-17 10:42:51.487734: W tensorflow/core/kernels/data/cache_dataset_ops.cc:858] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset  will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.\n",
      "2023-12-17 10:42:51.517752: W tensorflow/core/kernels/data/cache_dataset_ops.cc:858] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset  will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 9\u001b[0m\n\u001b[1;32m      6\u001b[0m     predictions \u001b[38;5;241m=\u001b[39m transformer(inp, tar, \u001b[38;5;28;01mTrue\u001b[39;00m, \u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[1;32m      7\u001b[0m     loss \u001b[38;5;241m=\u001b[39m loss_function(tar, predictions)\n\u001b[0;32m----> 9\u001b[0m gradients \u001b[38;5;241m=\u001b[39m \u001b[43mtape\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgradient\u001b[49m\u001b[43m(\u001b[49m\u001b[43mloss\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtransformer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrainable_variables\u001b[49m\u001b[43m)\u001b[49m    \n\u001b[1;32m     10\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mapply_gradients(\u001b[38;5;28mzip\u001b[39m(gradients, transformer\u001b[38;5;241m.\u001b[39mtrainable_variables))\n\u001b[1;32m     12\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m batch \u001b[38;5;241m%\u001b[39m \u001b[38;5;241m50\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n",
      "File \u001b[0;32m~/.virtualenvs/lib/python3.10/site-packages/tensorflow/python/eager/backprop.py:1066\u001b[0m, in \u001b[0;36mGradientTape.gradient\u001b[0;34m(self, target, sources, output_gradients, unconnected_gradients)\u001b[0m\n\u001b[1;32m   1060\u001b[0m   output_gradients \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m   1061\u001b[0m       composite_tensor_gradient\u001b[38;5;241m.\u001b[39mget_flat_tensors_for_gradients(\n\u001b[1;32m   1062\u001b[0m           output_gradients))\n\u001b[1;32m   1063\u001b[0m   output_gradients \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01mif\u001b[39;00m x \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m ops\u001b[38;5;241m.\u001b[39mconvert_to_tensor(x)\n\u001b[1;32m   1064\u001b[0m                       \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m output_gradients]\n\u001b[0;32m-> 1066\u001b[0m flat_grad \u001b[38;5;241m=\u001b[39m \u001b[43mimperative_grad\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mimperative_grad\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1067\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_tape\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1068\u001b[0m \u001b[43m    \u001b[49m\u001b[43mflat_targets\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1069\u001b[0m \u001b[43m    \u001b[49m\u001b[43mflat_sources\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1070\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_gradients\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_gradients\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1071\u001b[0m \u001b[43m    \u001b[49m\u001b[43msources_raw\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mflat_sources_raw\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1072\u001b[0m \u001b[43m    \u001b[49m\u001b[43munconnected_gradients\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43munconnected_gradients\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1074\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_persistent:\n\u001b[1;32m   1075\u001b[0m   \u001b[38;5;66;03m# Keep track of watched variables before setting tape to None\u001b[39;00m\n\u001b[1;32m   1076\u001b[0m   \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_watched_variables \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_tape\u001b[38;5;241m.\u001b[39mwatched_variables()\n",
      "File \u001b[0;32m~/.virtualenvs/lib/python3.10/site-packages/tensorflow/python/eager/imperative_grad.py:67\u001b[0m, in \u001b[0;36mimperative_grad\u001b[0;34m(tape, target, sources, output_gradients, sources_raw, unconnected_gradients)\u001b[0m\n\u001b[1;32m     63\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m:\n\u001b[1;32m     64\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m     65\u001b[0m       \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUnknown value for unconnected_gradients: \u001b[39m\u001b[38;5;132;01m%r\u001b[39;00m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m unconnected_gradients)\n\u001b[0;32m---> 67\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mpywrap_tfe\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mTFE_Py_TapeGradient\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     68\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtape\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_tape\u001b[49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# pylint: disable=protected-access\u001b[39;49;00m\n\u001b[1;32m     69\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtarget\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     70\u001b[0m \u001b[43m    \u001b[49m\u001b[43msources\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     71\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_gradients\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     72\u001b[0m \u001b[43m    \u001b[49m\u001b[43msources_raw\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     73\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcompat\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mas_str\u001b[49m\u001b[43m(\u001b[49m\u001b[43munconnected_gradients\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvalue\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.virtualenvs/lib/python3.10/site-packages/tensorflow/python/eager/backprop.py:118\u001b[0m, in \u001b[0;36m_gradient_function\u001b[0;34m(op_name, attr_tuple, num_inputs, inputs, outputs, out_grads, skip_input_indices, forward_pass_name_scope)\u001b[0m\n\u001b[1;32m    109\u001b[0m   \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_get_control_flow_context\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    110\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mNotImplementedError\u001b[39;00m(\n\u001b[1;32m    111\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtf.GradientTape.gradients() does not support graph control flow \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    112\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124moperations like tf.cond or tf.while at this time. Use tf.gradients() \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    113\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minstead. If you need this feature, please file a feature request at \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    114\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhttps://github.com/tensorflow/tensorflow/issues/new\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    115\u001b[0m     )\n\u001b[0;32m--> 118\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_gradient_function\u001b[39m(op_name, attr_tuple, num_inputs, inputs, outputs,\n\u001b[1;32m    119\u001b[0m                        out_grads, skip_input_indices, forward_pass_name_scope):\n\u001b[1;32m    120\u001b[0m \u001b[38;5;250m  \u001b[39m\u001b[38;5;124;03m\"\"\"Calls the gradient function of the op.\u001b[39;00m\n\u001b[1;32m    121\u001b[0m \n\u001b[1;32m    122\u001b[0m \u001b[38;5;124;03m  Args:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    134\u001b[0m \u001b[38;5;124;03m    The gradients with respect to the inputs of the function, as a list.\u001b[39;00m\n\u001b[1;32m    135\u001b[0m \u001b[38;5;124;03m  \"\"\"\u001b[39;00m\n\u001b[1;32m    136\u001b[0m   mock_op \u001b[38;5;241m=\u001b[39m _MockOp(attr_tuple, inputs, outputs, op_name, skip_input_indices)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "EPOCHS = 20\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    for (batch, (inp, tar)) in enumerate(train_dataset):\n",
    "        with tf.GradientTape() as tape:\n",
    "            predictions = transformer(inp, tar, True, None, None, None)\n",
    "            loss = loss_function(tar, predictions)\n",
    "\n",
    "        gradients = tape.gradient(loss, transformer.trainable_variables)    \n",
    "        optimizer.apply_gradients(zip(gradients, transformer.trainable_variables))\n",
    "\n",
    "        if batch % 50 == 0:\n",
    "            print(f'Epoch {epoch} Batch {batch} Loss {loss.numpy()}')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".virtualenvs",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
